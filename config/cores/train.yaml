root_folder: results/test/cores
seed: 0
device: cpu
plot: False
data: mutag
data_kwargs:
  root: ../Data
  splits: [0.5, 0.4, 0.1]
  k_fold: 2
  shuffle_train: True
  balance: None
  single_split: None
  preprocessing_dict_x:  {}
  preprocessing_dict_y:  {}
dataloader_kwargs:
  batch_size: 16
  num_workers: 0
checkpoint: disabled
checkpoint_kwargs:
  filename: "model_{epoch}"
  verbose: False
  save_last: True
  every_n_epochs: 1
  save_weights_only: False
early_stopping_kwargs:
  ppo_kwargs:
    metric: training/ppo_loss___min
    min_delta: 0.0001
    patience: 10
  clf_kwargs:
    metric: valid/graph_clf_loss_full
    mode: min
    min_delta: 0.0
    patience: 15
graph_env: multi
graph_env_kwargs:
  penalty_size: 1.0
  max_episode_length: 1
  use_intrinsic_reward: False
  device: ${device}
init_fn: xavier
init_fn_kwargs:
  nonlinearity: ${graph_clf_kwargs.activation}
  param: null
cores:
  env_steps: 128
  ppo_steps: 5
  action_refers_to: node
  metric_objective:
    name: valid/1__accuracy
    mode: max
  gnn_mode: False
graph_clf: gcn
graph_clf_kwargs:
  hidden_dim: 32
  activation: relu
  dropout: 0.1
  has_bn: False
  stage_type: skipsum
  layers_pre_num: 1
  layers_gnn_num: 2
  layers_post_num: 1
  pooling: [mean, add]
  device: ${device}
logger: dummy
logger_kwargs:
  enable: False
metrics: clf_binary
metrics_kwargs:
  full_names: ["accuracy"]
  device: ${device}
lr_scheduler_clf: plateau
lr_scheduler_kwargs_clf:
  mode: max
  factor: 0.99
  patience: 20
  min_lr: 0.0001
lr_scheduler_rl: plateau
lr_scheduler_kwargs_rl:
  factor: ${lr_scheduler_kwargs_clf.factor}
  patience: ${lr_scheduler_kwargs_clf.patience}
  min_lr: ${lr_scheduler_kwargs_clf.min_lr}
loss: bce_logits
loss_kwargs:
  reduction: mean
optimizer_clf: adam
optimizer_kwargs_clf:
  lr: 0.001
  weight_decay: 0.0001
  betas: [0.9, 0.999]
optimizer_rl: adam
optimizer_kwargs_rl:
  ratio_clf: 1.5
  lr_clf: ${optimizer_kwargs_clf.lr}
  ratio_critic: 1.5
policy: graph_actor_critic
policy_kwargs:
  pool_type: [mean, add]
  activation: ${graph_clf_kwargs.activation}
ppo: ppo_graph
ppo_kwargs:
  eps_clip: 0.2
  gamma: 1.0
  coeff_mse: 1.0
  coeff_entropy: 0.01
reward: cores
reward_kwargs:
  device: ${device}
trainer:
  max_epochs: 100
  limit_val_batches: 1.0

